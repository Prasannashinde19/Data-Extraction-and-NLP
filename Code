import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')
import re

df = pd.read_excel('Input.xlsx')

import os
import pandas as pd
from bs4 import BeautifulSoup
import requests
from requests.exceptions import RequestException, ConnectionError, HTTPError

def fetch_url(url, max_retries=3):
    for _ in range(max_retries):
        try:
            response = requests.get(url)
            response.raise_for_status()
            return response

        except HTTPError as e:
            if "404" in str(e):
                print(f"URL not found (404): {url}")
                return None  # Skip further retries for 404 errors

            print(f"HTTPError fetching {url}: {e}")

        except (RequestException, ConnectionError) as e:
            print(f"Error fetching {url}: {e}")

    return None

# Create a folder named "TitleText" if it doesn't exist
output_folder = "TitleText"
os.makedirs(output_folder, exist_ok=True)

# Read Input.xlsx
input_df = pd.read_excel("Input.xlsx")

# Web Scraping
for index, row in input_df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    # Fetch the URL with retries
    response = fetch_url(url)

    if response:
        try:
            # Parse the HTML content
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract article title and text
            article_title = soup.title.text if soup.title else ''
            article_text = ' '.join([p.text for p in soup.find_all('p')])

            # Save extracted data to a text file in the "TitleText" folder
            file_path = os.path.join(output_folder, f"{url_id}.txt")
            with open(file_path, 'w', encoding='utf-8') as file:
                file.write(f"{article_title}\n{article_text}")

        except Exception as e:
            print(f"Error processing {url}: {e}")


#find text
article = ""
try:
    for p in soup.find_all('p'):
        article += p.get_text()
except:
    print("can't get text of {}".format(url_id))

#write title and text to the file
file_name = r"C:\Users\prasa\AppData\Local\Programs\Python\Python311\Scripts\NLP\TitleText" + str(url_id) + '.txt'
with open(file_name, 'w') as file:
    file.write(article_title + '\n' + article)

# Directories
text_dir = r"C:\Users\prasa\AppData\Local\Programs\Python\Python311\Scripts\NLP\TitleText"
stopwords_dir = r"C:\Users\prasa\AppData\Local\Programs\Python\Python311\Scripts\NLP\StopWords"
sentment_dir = r"C:\Users\prasa\AppData\Local\Programs\Python\Python311\Scripts\NLP\MasterDictionary"

# load all stop wors from the stopwords directory and store in the set variable
stop_words = set()
for files in os.listdir(stopwords_dir):
    with open(os.path.join(stopwords_dir,files),'r',encoding='ISO-8859-1') as f:
        stop_words.update(set(f.read().splitlines()))

# load all text files  from the  directory and store in a list(docs)
docs = []
for text_file in os.listdir(text_dir):
    with open(os.path.join(text_dir, text_file), 'r', encoding='ISO-8859-1') as file:
        text = file.read()
#tokenize the given text file
    words = word_tokenize(text)
# remove the stop words from the tokens
    filtered_text = [word for word in words if word.lower() not in stop_words]
# add each filtered tokens of each file into a list
    docs.append(filtered_text)

# store positive, Negative words from the directory
pos=set()
neg=set()

for files in os.listdir(sentment_dir):
    if files =='positive-words.txt':
        with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:
              pos.update(f.read().splitlines())
    else:
        with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:
              neg.update(f.read().splitlines())

# now collect the positive  and negative words from each file
# calculate the scores from the positive and negative words 
positive_words = []
negative_words =[]
positive_score = []
negative_score = []
polarity_score = []
subjectivity_score = []

#iterate through the list of docs
for i in range(len(docs)):
    positive_words.append([word for word in docs[i] if word.lower() in pos])
    negative_words.append([word for word in docs[i] if word.lower() in neg])
    positive_score.append(len(positive_words[i]))
    negative_score.append(len(negative_words[i]))
    polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))
    subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))

# Display or use the collected data as needed
print("Positive Words:", positive_words)
print("Negative Words:", negative_words)
print("Positive Scores:", positive_score)
print("Negative Scores:", negative_score)
print("Polarity Scores:", polarity_score)
print("Subjectivity Scores:", subjectivity_score)

# Average Sentence Length = the number of words / the number of sentences
# Percentage of Complex words = the number of complex words / the number of words 
# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)

avg_sentence_length = []
Percentage_of_Complex_words  =  []
Fog_Index = []
complex_word_count =  []
avg_syllable_word_count =[]

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

stopwords_set = set(stopwords.words('english'))


def measure(file):
    with open(os.path.join(text_dir, file), 'r', encoding='utf-8') as f:
        text = f.read()
# remove punctuations 
    text = re.sub(r'[^\w\s.]','',text)
# split the given text file into sentences
    sentences = text.split('.')
# total number of sentences in a file
    num_sentences = len(sentences)
# total words in the file
    words = [word  for word in text.split() if word.lower() not in stopwords_set ]
    num_words = len(words)
 
 # complex words having syllable count is greater than 2
# Complex words are words in the text that contain more than two syllables.
    complex_words = []
    for word in words:
        vowels = 'aeiou'
        syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)
        if syllable_count_word > 2:
            complex_words.append(word)

# Syllable Count Per Word
# We count the number of Syllables in each word of the text by counting the vowels present in each word.
#  We also handle some exceptions like words ending with "es","ed" by not counting them as a syllable.
    syllable_count = 0
    syllable_words =[]
    for word in words:
        if word.endswith('es'):
            word = word[:-2]
        elif word.endswith('ed'):
            word = word[:-2]
            vowels = 'aeiou'
            syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)
        if syllable_count_word >= 1:
            syllable_words.append(word)
            syllable_count += syllable_count_word


    avg_sentence_len = num_words / num_sentences
    avg_syllable_word_count = syllable_count / len(syllable_words)
    Percent_Complex_words  =  len(complex_words) / num_words
    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)

    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count


# iterate through each file or doc
for file in os.listdir(text_dir):
    x,y,z,a,b = measure(file)
    avg_sentence_length.append(x)
    Percentage_of_Complex_words.append(y)
    Fog_Index.append(z)
    complex_word_count.append(a)
    avg_syllable_word_count.append(b)

def cleaned_words(file):
    with open(os.path.join(text_dir,file), 'r' , encoding='utf-8', errors='ignore') as f:
        text = f.read()
        text = re.sub(r'[^\w\s]', '' , text)
        words = [word  for word in text.split() if word.lower() not in stopwords_set]
        length = sum(len(word) for word in words)
        average_word_length = length / len(words)
    return len(words),average_word_length

word_count = []
average_word_length = []
for file in os.listdir(text_dir):
    x, y = cleaned_words(file)
    word_count.append(x)
    average_word_length.append(y)

# To calculate Personal Pronouns mentioned in the text, we use regex to find 
# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken
#  so that the country name US is not included in the list.
def count_personal_pronouns(file):
    with open(os.path.join(text_dir,file), 'r' , encoding='utf-8', errors='ignore') as f:
        text = f.read()
        personal_pronouns = ["I", "we", "my", "ours", "us"]
        count = 0
    for pronoun in personal_pronouns:
        count += len(re.findall(r"\b" + pronoun + r"\b", text)) # \b is used to match word boundaries
    return count

pp_count = []
for file in os.listdir(text_dir):
    x = count_personal_pronouns(file)
    pp_count.append(x)

output_df = pd.read_excel('Output Data Structure.xlsx')

# URL_ID 36 , 49  does not exists i,e. page does not exist, throughs 404 error
# so we are going to drop these rows from the table
output_df.drop([36 , 49], axis = 0, inplace=True)

variables = [positive_score,
            negative_score,
            polarity_score,
            subjectivity_score,
            avg_sentence_length,
            Percentage_of_Complex_words,
            Fog_Index,
            avg_sentence_length,
            complex_word_count,
            word_count,
            avg_syllable_word_count,
            pp_count,
            average_word_length]

df = pd.DataFrame(variables)
df

transposed_df = df.transpose()
transposed_df

df.columns = variables

transposed_df

# write the values to the dataframe
for i, var in enumerate(variables):
    output_df.iloc[:,i+2] = var

output_df.to_csv('Output_Data.csv')
